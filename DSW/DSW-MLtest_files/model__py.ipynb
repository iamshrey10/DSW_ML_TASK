{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-f6QkNnYSBY_jORU9UwbKtot-x-36sUh",
      "authorship_tag": "ABX9TyMY/1vTCHr//3P7YXPQdGYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GouravMidya/DSW-MLtest/blob/main/model__py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "e3-kio8ZaVMY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "L-qIxlAoYS3Z"
      },
      "outputs": [],
      "source": [
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def load(self, train_filepath, test_filepath):\n",
        "        self.train_data = pd.read_excel(train_filepath)\n",
        "        self.test_data = pd.read_excel(test_filepath)\n",
        "        print(\"Training and testing data loaded successfully.\")\n",
        "\n",
        "    def preprocess(self):\n",
        "        def process_data(data):\n",
        "            # Feature engineering for transaction_date\n",
        "            data['transaction_date'] = pd.to_datetime(data['transaction_date'])\n",
        "            data['transaction_year'] = data['transaction_date'].dt.year\n",
        "            data['transaction_month'] = data['transaction_date'].dt.month\n",
        "\n",
        "            # Drop unnecessary columns\n",
        "            data = data.drop(['customer_id', 'transaction_date'], axis=1)\n",
        "\n",
        "            # Encode categorical variables\n",
        "            categorical_cols = ['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type', 'verification_status']\n",
        "            for col in categorical_cols:\n",
        "                if col not in self.label_encoders:\n",
        "                    le = LabelEncoder()\n",
        "                    data[col] = le.fit_transform(data[col])\n",
        "                    self.label_encoders[col] = le\n",
        "                else:\n",
        "                    data[col] = self.label_encoders[col].transform(data[col])\n",
        "\n",
        "            # Scale numerical features\n",
        "            numerical_cols = ['cibil_score', 'total_no_of_acc', 'annual_inc', 'int_rate',\n",
        "                              'loan_amnt', 'installment', 'account_bal', 'emp_length', 'transaction_year', 'transaction_month']\n",
        "            data[numerical_cols] = self.scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "            return data\n",
        "\n",
        "        self.train_data = process_data(self.train_data)\n",
        "        self.test_data = process_data(self.test_data)\n",
        "        print(\"Data preprocessing completed.\")\n",
        "\n",
        "    def split_data(self):\n",
        "        X_train = self.train_data.drop('loan_status', axis=1)\n",
        "        y_train = self.train_data['loan_status']\n",
        "        X_test = self.test_data.drop('loan_status', axis=1)\n",
        "        y_test = self.test_data['loan_status']\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        raise NotImplementedError(\"Train method must be implemented by subclasses.\")\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(\"Classification Report:\\n\", report)\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionModel(BaseModel):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "        print(\"Logistic Regression model trained successfully.\")\n",
        "\n",
        "class XGBoostModel(BaseModel):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=1)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "        print(\"XGBoost model trained successfully.\")"
      ],
      "metadata": {
        "id": "qzE4jeC_acGM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example pipeline usage\n",
        "if __name__ == \"__main__\":\n",
        "    train_filepath = \"/content/drive/MyDrive/DSW Assessment/train_data.xlsx\"  # Replace with the actual training data file\n",
        "    test_filepath = \"/content/drive/MyDrive/DSW Assessment/test_data.xlsx\"    # Replace with the actual testing data file\n",
        "\n",
        "    # Logistic Regression pipeline\n",
        "    lr_model = LogisticRegressionModel()\n",
        "    lr_model.load(train_filepath, test_filepath)\n",
        "    lr_model.preprocess()\n",
        "    X_train, X_test, y_train, y_test = lr_model.split_data()\n",
        "    lr_model.train(X_train, y_train)\n",
        "    lr_model.test(X_test, y_test)\n",
        "\n",
        "    # XGBoost pipeline\n",
        "    xgb_model = XGBoostModel()\n",
        "    xgb_model.load(train_filepath, test_filepath)\n",
        "    xgb_model.preprocess()\n",
        "    X_train, X_test, y_train, y_test = xgb_model.split_data()\n",
        "    xgb_model.train(X_train, y_train)\n",
        "    xgb_model.test(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJzqKSX4aeXH",
        "outputId": "95b43f47-00d9-472e-89dd-1f59ead1a7aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and testing data loaded successfully.\n",
            "Data preprocessing completed.\n",
            "Logistic Regression model trained successfully.\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.53      0.53      3055\n",
            "           1       0.73      0.74      0.74      5400\n",
            "\n",
            "    accuracy                           0.66      8455\n",
            "   macro avg       0.63      0.63      0.63      8455\n",
            "weighted avg       0.66      0.66      0.66      8455\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1610 1445]\n",
            " [1398 4002]]\n",
            "Training and testing data loaded successfully.\n",
            "Data preprocessing completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:51:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model trained successfully.\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.17      0.27      3055\n",
            "           1       0.67      0.95      0.79      5400\n",
            "\n",
            "    accuracy                           0.67      8455\n",
            "   macro avg       0.67      0.56      0.53      8455\n",
            "weighted avg       0.67      0.67      0.60      8455\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 507 2548]\n",
            " [ 244 5156]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided evaluation metrics and the problem statement, the **XGBoost model** appears to be the better choice. Here's why:\n",
        "\n",
        "### Context of the Use Case\n",
        "The goal is to predict loan repayment behavior, particularly identifying potential defaulters. This is a highly sensitive use case where identifying defaulters (positive class `1`) accurately is critical to minimize financial risk.\n",
        "\n",
        "### Evaluation Comparison\n",
        "1. **Precision**:\n",
        "   - Logistic Regression (Class `1`): **0.73**\n",
        "   - XGBoost (Class `1`): **0.67**\n",
        "\n",
        "   Logistic Regression has a slightly better precision, meaning it is less likely to falsely classify a non-defaulter as a defaulter.\n",
        "\n",
        "2. **Recall (Sensitivity)**:\n",
        "   - Logistic Regression (Class `1`): **0.74**\n",
        "   - XGBoost (Class `1`): **0.95**\n",
        "\n",
        "   XGBoost significantly outperforms Logistic Regression in recall, which is crucial for this use case. High recall ensures that most actual defaulters are identified, reducing the risk of approving loans to defaulters.\n",
        "\n",
        "3. **F1-Score**:\n",
        "   - Logistic Regression (Class `1`): **0.74**\n",
        "   - XGBoost (Class `1`): **0.79**\n",
        "\n",
        "   The F1-Score balances precision and recall. XGBoost has a higher F1-Score, indicating better overall performance in identifying defaulters.\n",
        "\n",
        "4. **Overall Accuracy**:\n",
        "   - Logistic Regression: **0.66**\n",
        "   - XGBoost: **0.67**\n",
        "\n",
        "   XGBoost has slightly better overall accuracy, though this metric is less important given the class imbalance.\n",
        "\n",
        "5. **Confusion Matrix**:\n",
        "   - XGBoost identifies a higher number of defaulters (`5156/5400`) compared to Logistic Regression (`4002/5400`). This aligns with the goal of minimizing undetected defaulters.\n",
        "\n",
        "### Final Decision\n",
        "While Logistic Regression has slightly better precision, the XGBoost model's significantly higher recall and F1-Score make it the preferred choice for this use case, where missing defaulters is a more critical issue than occasionally misclassifying a non-defaulter.\n"
      ],
      "metadata": {
        "id": "aWZbuzlfbquF"
      }
    }
  ]
}